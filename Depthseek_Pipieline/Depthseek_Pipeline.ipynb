{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depthseek Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you had completed sort your sample data by col1.\n",
    "If didnot, please use the following command in your terminal:\n",
    "\n",
    "\"sort -S 50% --parallel 16 -k1,1 /path/validpairs > /output_path/sorted.validpairs\",\n",
    "\n",
    "then let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, right. For the small samples in the early sequencing stage, the sorting process won't take long. If you want to verify with samples that have a deeper sequencing depth, \n",
    "\n",
    "the sort process may take longer. We recommend that when Depthseek is used for deep sequencing samples, it is best not to exceed the range of predicting 200million from \n",
    "\n",
    "15 million reads. \n",
    "\n",
    "You know, this is indeed a kind of limit. hahah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries for Depthseek Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import subprocess\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import warnings\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Global Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 6\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"./Demo/test_data/SRR10093262.sorted.validPairs\" \n",
    "output_dir = \"./Demo/test_output\" \n",
    "temp_dir = \"./Demo/test_tmp\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Parameters for Processing and Predicting tasks of Depthseek Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_threads = 64\n",
    "sort_memory = \"6G\"\n",
    "max_parallel_jobs = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for Predicting\n",
    "\n",
    "When the prediction task is Forward prediction, what is executed is the library complexity of predicting the Target value based on the data. Conversely, when the task values are Inverse, the task is to predict the depth of library sequencing required for the Target (in this case, it is the library complexity or the number of effective interactions) based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_mode = \"forward\"  ##TODO Modeï¼š\"forward\" or \"inverse\"\n",
    "target_value = 150          ##TODO Target: forward mode is Million reads, while inverse mode is reads.(Important!)\n",
    "model_type = \"depthseek\"         ##TODO Formula: \"lw\", \"depthseek\", or \"both\". (lw is Lander-Waterman Formula, depthseek is Self-adaptive lw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analog sampling sequencing (FIFO downsampling and deduplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fifo_downsample_dedup(input_file, output_dir, temp_dir, sort_threads, sort_memory, max_parallel_jobs):\n",
    "    \"\"\"Run the FIFO downsampling and deduplication process\"\"\"\n",
    "    \n",
    "    def validate_args(args):\n",
    "        if not os.path.isfile(args.i):\n",
    "            sys.exit(f\"Error: Input file {args.i} does not exist\")\n",
    "        os.makedirs(args.o, exist_ok=True)\n",
    "        os.makedirs(args.t, exist_ok=True)\n",
    "\n",
    "    def run_task(line, args, sample_name):\n",
    "        start_time = time.time()\n",
    "        sample_tmp_dir = os.path.join(args.t, sample_name, str(line))\n",
    "        os.makedirs(sample_tmp_dir, exist_ok=True)\n",
    "        \n",
    "        fifo_path = os.path.join(sample_tmp_dir, 'data.fifo')\n",
    "        try:\n",
    "            os.mkfifo(fifo_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        # Extract the first N lines to the FIFO\n",
    "        head_cmd = f\"head -n {line} {args.i} > {fifo_path}\"\n",
    "        head_proc = subprocess.Popen(head_cmd, shell=True)\n",
    "        \n",
    "        # Process Data\n",
    "        sort_cmd = [\n",
    "            'sort',\n",
    "            f'--parallel={args.p}',\n",
    "            f'-T', sample_tmp_dir,\n",
    "            f'-S', args.s,\n",
    "            '-k2,2V',\n",
    "            '-k3,3n',\n",
    "            '-k5,5V',\n",
    "            '-k6,6n',\n",
    "            fifo_path\n",
    "        ]\n",
    "        \n",
    "        awk_cmd = [\n",
    "            'awk',\n",
    "            '-F\\t',\n",
    "            'BEGIN{c1=0;c2=0;s1=0;s2=0}(c1!=$2 || c2!=$5 || s1!=$3 || s2!=$6){print;c1=$2;c2=$5;s1=$3;s2=$6}'\n",
    "        ]\n",
    "        \n",
    "        wc_cmd = ['wc', '-l']\n",
    "        \n",
    "        # Build a processing Pipe\n",
    "        sort_proc = subprocess.Popen(sort_cmd, stdout=subprocess.PIPE)\n",
    "        awk_proc = subprocess.Popen(awk_cmd, stdin=sort_proc.stdout, stdout=subprocess.PIPE)\n",
    "        wc_proc = subprocess.Popen(wc_cmd, stdin=awk_proc.stdout, stdout=subprocess.PIPE)\n",
    "        \n",
    "        # Obtain the final result\n",
    "        dedup_lines = wc_proc.communicate()[0].decode().strip()\n",
    "        \n",
    "        # Generate the output file\n",
    "        data_file = os.path.join(args.o, f\"{sample_name}_{line}_data.txt\")\n",
    "        with open(data_file, 'w') as f:\n",
    "            normalized_line = line // 1000000\n",
    "            f.write(f\"{normalized_line}\\t{dedup_lines}\\n\")\n",
    "        \n",
    "        # Record the running time\n",
    "        runtime = int(time.time() - start_time)\n",
    "        runtime_log = os.path.join(args.o, f\"{sample_name}_runtime_log.txt\")\n",
    "        with open(runtime_log, 'a') as f:\n",
    "            f.write(f\"Task {line} completed in {runtime} seconds\\n\")\n",
    "        \n",
    "        # Clean up temporary files\n",
    "        head_proc.wait()\n",
    "        os.remove(fifo_path)\n",
    "        os.rmdir(sample_tmp_dir)\n",
    "\n",
    "    # Create parameter objects\n",
    "    class Args:\n",
    "        def __init__(self, i, o, t, p, s, f):\n",
    "            self.i = i\n",
    "            self.o = o\n",
    "            self.t = t\n",
    "            self.p = p\n",
    "            self.s = s\n",
    "            self.f = f\n",
    "    \n",
    "    args = Args(input_file, output_dir, temp_dir, sort_threads, sort_memory, max_parallel_jobs)\n",
    "    validate_args(args)\n",
    "    \n",
    "    # Extract sample name\n",
    "    file_name = os.path.basename(args.i)\n",
    "    sample_name = file_name.split('_sorted')[0]\n",
    "    \n",
    "    # Generate task list\n",
    "    tasks = [i * 1000000 for i in range(1, 21)]  ##TODO range(1,x) should be changed for your data sample.\n",
    "    \n",
    "    # Parallel processing tasks\n",
    "    with ThreadPoolExecutor(max_workers=args.f) as executor:\n",
    "        futures = []\n",
    "        for line in tasks:\n",
    "            futures.append(executor.submit(run_task, line, args, sample_name))\n",
    "        \n",
    "        # Wait for all tasks to be completed\n",
    "        for future in futures:\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Task failed: {e}\")\n",
    "    \n",
    "    # Merge result files\n",
    "    combined_data = []\n",
    "    for line in tasks:\n",
    "        data_file = os.path.join(args.o, f\"{sample_name}_{line}_data.txt\")\n",
    "        if os.path.exists(data_file):\n",
    "            with open(data_file) as f:\n",
    "                combined_data.append(f.read().strip())\n",
    "    \n",
    "    # Sort and write to the final file\n",
    "    final_output = os.path.join(args.o, f\"{sample_name}_final_output.txt\")\n",
    "    sorted_data = sorted(combined_data, key=lambda x: int(x.split('\\t')[0]))\n",
    "    with open(final_output, 'w') as f:\n",
    "        f.write(\"\\n\".join(sorted_data))\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for line in tasks:\n",
    "        data_file = os.path.join(args.o, f\"{sample_name}_{line}_data.txt\")\n",
    "        if os.path.exists(data_file):\n",
    "            os.remove(data_file)\n",
    "\n",
    "    runtime_log = os.path.join(args.o, f\"{sample_name}_runtime_log.txt\")\n",
    "    max_runtime = 0\n",
    "    if os.path.exists(runtime_log):\n",
    "        with open(runtime_log) as f:\n",
    "            for line in f:\n",
    "                if 'completed' in line:\n",
    "                    rt = int(line.split()[-2])\n",
    "                    max_runtime = max(max_runtime, rt)\n",
    "        with open(runtime_log, 'a') as f:\n",
    "            f.write(f\"Longest runtime: {max_runtime} seconds\\n\")\n",
    "    \n",
    "    print(f\"Results saved to {final_output}\")\n",
    "    print(f\"Longest runtime: {max_runtime} seconds\")\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Analog Sampling Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statr -- Analog Sampling Sequencing...\")\n",
    "final_output_file = run_fifo_downsample_dedup(input_file, output_dir, temp_dir, sort_threads, sort_memory, max_parallel_jobs)\n",
    "print(\"Finished -- Analog Sampling Sequencing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loading and Preprocessing data\"\"\"\n",
    "    data = np.loadtxt(file_path)\n",
    "    X = data[:, 0].reshape(-1, 1)\n",
    "    y = data[:, 1].reshape(-1, 1)\n",
    "    \n",
    "    scaler_X = StandardScaler()\n",
    "    X = scaler_X.fit_transform(X)\n",
    "    \n",
    "    scaler_y = StandardScaler()\n",
    "    y = scaler_y.fit_transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test, scaler_X, scaler_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type):\n",
    "    \"\"\"Building the prediction model\"\"\"  ##TODO Make a choice~(although we had tested for many times, the results is similar )\n",
    "    if model_type == 'lasso':\n",
    "        model = Lasso(alpha=0.1, random_state=42)\n",
    "    elif model_type == 'random_forest':\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    elif model_type == 'xgboost':\n",
    "        model = XGBRegressor(random_state=42)\n",
    "    elif model_type == 'svm':\n",
    "        model = SVR(kernel='rbf')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train):\n",
    "    \"\"\"Training\"\"\"\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate the Models\"\"\"\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    \n",
    "    train_loss = mean_squared_error(y_train, train_pred)\n",
    "    test_loss = mean_squared_error(y_test, test_pred)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss}\")\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    return train_loss, test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_function_1(x, m):\n",
    "    \"\"\"lw\"\"\"\n",
    "    return m - m * np.exp(-1000000 * x / m)\n",
    "\n",
    "def custom_function_2(x, a, m):\n",
    "    \"\"\"Depthseek\"\"\"  # self-adaptive lw\n",
    "    return m - m * np.exp(-((1000000*x)**a) / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the inverse functions\n",
    "def inverse_function_1(y, m):\n",
    "    \"\"\"inverse - lw\"\"\"\n",
    "    y_clipped = np.minimum(y, m * 0.999)\n",
    "    return (-m / 1000000) * np.log(1 - y_clipped / m)\n",
    "\n",
    "def inverse_function_2(y, a, m):\n",
    "    \"\"\"inverse - Depthseek\"\"\"\n",
    "    y_clipped = np.minimum(y, m * 0.999)\n",
    "    inner = -m * np.log(1 - y_clipped / m)\n",
    "    return (inner ** (1/a)) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_parameters(X, y, y_max):\n",
    "    \"\"\"Parameter estimation optimization\"\"\"\n",
    "    # lw -- for m -- Theoretical sequencing library complexity\n",
    "    try:\n",
    "        initial_guess_1 = [y_max * 0.8]\n",
    "        bounds_1 = ([0.5*y_max], [np.inf])\n",
    "        popt_1, _ = curve_fit(custom_function_1, \n",
    "                             X.ravel(), \n",
    "                             y.ravel(),\n",
    "                             p0=initial_guess_1,\n",
    "                             bounds=bounds_1,\n",
    "                             method='trf',\n",
    "                             maxfev=10000)\n",
    "        m_est_1 = popt_1[0]\n",
    "    except RuntimeError:\n",
    "        m_est_1 = y_max\n",
    "\n",
    "    # self-adaptive-lw -- for a and m -- a Library specific parameters,m Theoretical sequencing library complexity\n",
    "    try:\n",
    "        initial_guess_2 = [0.95, y_max*0.9]\n",
    "        bounds_2 = ([0.8, 0.7*y_max], [1.2, np.inf])\n",
    "        popt_2, _ = curve_fit(custom_function_2,\n",
    "                             X.ravel(),\n",
    "                             y.ravel(),\n",
    "                             p0=initial_guess_2,\n",
    "                             bounds=bounds_2,\n",
    "                             method='trf',\n",
    "                             maxfev=10000)\n",
    "        a_est_2, m_est_2 = popt_2\n",
    "    except RuntimeError:\n",
    "        a_est_2, m_est_2 = 1.0, y_max\n",
    "\n",
    "    return m_est_1, a_est_2, m_est_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deviation(y_true, y_pred):\n",
    "    \"\"\"Calculation deviation\"\"\"\n",
    "    deviation = np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "    return deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inverse_deviation(X_true, X_pred):\n",
    "    \"\"\"Calculation deviation for inverse mode\"\"\"\n",
    "    valid_indices = (X_true > 1) & (X_pred > 1)\n",
    "    if np.any(valid_indices):\n",
    "        deviation = np.mean(np.abs((X_true[valid_indices] - X_pred[valid_indices]) / X_true[valid_indices]))\n",
    "    else:\n",
    "        deviation = np.mean(np.abs((X_true - X_pred) / np.maximum(X_true, 1)))\n",
    "    return deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fitted_curve(X, y, m_est_1, a_est_2, m_est_2, scaler_X, scaler_y, label, color, ax1, ax2):\n",
    "    \"\"\"Plot the fit curve\"\"\"\n",
    "    X = scaler_X.inverse_transform(X)\n",
    "    y = scaler_y.inverse_transform(y)\n",
    "    \n",
    "    x_fit = np.linspace(0, min(X.max(), 1200), 100)\n",
    "    y_fit_1 = custom_function_1(x_fit, m_est_1)\n",
    "    y_fit_2 = custom_function_2(x_fit, a_est_2, m_est_2)\n",
    "    \n",
    "    ax1.plot(x_fit, y_fit_1, color='#1f77b4', linestyle='--', label=f'LW Model')\n",
    "    ax2.plot(x_fit, y_fit_2, color='#ff7f0e', linestyle='-.', label=f'Depthseek Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_fitted_curve(X, y, a_est, m_est, scaler_X, scaler_y, output_dir, dataset_name):\n",
    "    \"\"\"Plot one-fit curves (for HTML reporting)\"\"\"\n",
    "    X = scaler_X.inverse_transform(X)\n",
    "    y = scaler_y.inverse_transform(y)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5/2.54, 5/2.54))\n",
    "    \n",
    "    # Plot the raw data points\n",
    "    ax.scatter(X, y, s=5, color='#1E90FF', alpha=0.6, label='Observed Data')\n",
    "    \n",
    "    # Plot the fit curve\n",
    "    x_fit = np.linspace(0, min(X.max(), 1200), 100)\n",
    "    y_fit = custom_function_2(x_fit, a_est, m_est)\n",
    "    ax.plot(x_fit, y_fit, color='#EB5B25', linestyle='-', linewidth=1, label='Fitted Curve')\n",
    "    \n",
    "    # Set graph properties\n",
    "    ax.set_xlim(0, 800)\n",
    "    ax.set_xlabel('Sequencing Depth (Million Reads)', fontsize=6)\n",
    "    ax.set_ylabel('Unique Reads', fontsize=6)\n",
    "    \n",
    "    # Set the tick and border\n",
    "    ax.tick_params(axis='both', which='major', direction='in', length=3, width=0.5)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(0.5)\n",
    "    \n",
    "    # Add formula text\n",
    "    formula = r'$y = m - m \\cdot \\exp\\left(-\\frac{x^a}{m}\\right)$'\n",
    "    ax.text(0.95, 0.075, formula, \n",
    "        transform=ax.transAxes, \n",
    "        fontsize=5,\n",
    "        horizontalalignment='right',\n",
    "        verticalalignment='bottom'\n",
    "        )\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='lower right', \n",
    "          fontsize=5,\n",
    "          framealpha=0.8,\n",
    "          bbox_to_anchor=(0.95, 0.125),\n",
    "          facecolor='white',\n",
    "          edgecolor='none',\n",
    "          borderpad=0.3)\n",
    "    \n",
    "    # save the output svg file\n",
    "    output_svg = os.path.join(output_dir, f\"{dataset_name}_fit_result.svg\")\n",
    "    plt.savefig(output_svg, format='svg', bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html_report(output_dir, dataset_name, a_est, m_est, y_pred_n, deviation, y_max, n_value):\n",
    "    \"\"\"Generate HTML\"\"\"\n",
    "    html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Fitting Report - {dataset_name}</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; font-size: 10px; }}\n",
    "        .container {{ width: 80%; margin: 0 auto; }}\n",
    "        .image-container {{ text-align: center; margin: 10px 0; }}\n",
    "        .results {{ margin: 15px 0; }}\n",
    "        table {{ width: 100%; border-collapse: collapse; }}\n",
    "        th, td {{ border: 1px solid #ddd; padding: 5px; text-align: left; }}\n",
    "        th {{ background-color: #f2f2f2; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h2>Fitting Report - {dataset_name}</h2>\n",
    "        \n",
    "        <div class=\"image-container\">\n",
    "            <img src=\"{dataset_name}_fit_result.svg\" alt=\"Fitting Result\" style=\"width: 5cm; height: 5cm;\">\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"results\">\n",
    "            <h3>Fitting Results</h3>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Parameter</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Max observed y</td>\n",
    "                    <td>{y_max:.2f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Parameter a</td>\n",
    "                    <td>{a_est:.3f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Parameter m</td>\n",
    "                    <td>{m_est:.2f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Predicted value at {n_value}M</td>\n",
    "                    <td>{y_pred_n:.2f}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Deviation</td>\n",
    "                    <td>{deviation:.2%}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "    \"\"\"\n",
    "    html_file = os.path.join(output_dir, f\"{dataset_name}_report.html\")\n",
    "    with open(html_file, 'w') as f:\n",
    "        f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_analysis(input_file, output_dir, prediction_mode, target_value, model_type):\n",
    "    \"\"\"Operation prediction analysis\"\"\"\n",
    "    dataset_name = os.path.basename(input_file).replace('.txt', '')\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test, scaler_X, scaler_y = load_and_preprocess_data(input_file)\n",
    "    X_train_inv = scaler_X.inverse_transform(X_train)\n",
    "    y_train_inv = scaler_y.inverse_transform(y_train)\n",
    "    X_test_inv = scaler_X.inverse_transform(X_test)\n",
    "    y_test_inv = scaler_y.inverse_transform(y_test)\n",
    "    \n",
    "    # Estimated parameters\n",
    "    y_max = np.max(y_train_inv)\n",
    "    m_est_1, a_est_2, m_est_2 = estimate_parameters(X_train_inv, y_train_inv, y_max)\n",
    "    \n",
    "    # Forward Mode (x -> y)\n",
    "    if prediction_mode == \"forward\":\n",
    "        # XGBoost\n",
    "        model = build_model('xgboost')\n",
    "        model = train_model(model, X_train, y_train)\n",
    "        train_loss, test_loss = evaluate_model(model, X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Both formulas\n",
    "        y_pred_1 = custom_function_1(target_value, m_est_1)\n",
    "        y_pred_2 = custom_function_2(target_value, a_est_2, m_est_2)\n",
    "        \n",
    "        # Evaluate function performance on the test set\n",
    "        test_pred_1 = custom_function_1(X_test_inv.ravel(), m_est_1)\n",
    "        test_pred_2 = custom_function_2(X_test_inv.ravel(), a_est_2, m_est_2)\n",
    "        deviation_1 = calculate_deviation(y_test_inv.ravel(), test_pred_1)\n",
    "        deviation_2 = calculate_deviation(y_test_inv.ravel(), test_pred_2)\n",
    "        \n",
    "        # Select the output based on the model type\n",
    "        if model_type == \"lw\":\n",
    "            print(f\"LW Model prediction: At the sequencing depth of {target_value}M reads, the estimated library complexity is {y_pred_1:.2f}\")\n",
    "            print(f\"LW Model deviation: {deviation_1: 0.2%}\")\n",
    "\n",
    "            # Write in Logs\n",
    "            log_file = os.path.join(output_dir, f\"{dataset_name}_lw_log.txt\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"Max observed y: {y_max:.2f}\\n\")\n",
    "                f.write(f\"LW Model params: m={m_est_1:.2f}\\n\")\n",
    "                f.write(f\"Predicted value at {target_value}M: {y_pred_1:.2f}\\n\")\n",
    "                f.write(f\"Deviation: {deviation_1:.2%}\\n\")\n",
    "                \n",
    "        elif model_type == \"depthseek\":\n",
    "            print(f\"Depthseek Model prediction: At the sequencing depth of {target_value}M reads, the estimated library complexity is {y_pred_2:.2f}\")\n",
    "            print(f\"Depthseek Model deviation: {deviation_2:.2%}\")       \n",
    "\n",
    "            # Generate the HTML file\n",
    "            plot_single_fitted_curve(X_train, y_train, a_est_2, m_est_2, scaler_X, scaler_y, output_dir, dataset_name)\n",
    "            generate_html_report(output_dir, dataset_name, a_est_2, m_est_2, y_pred_2, deviation_2, y_max, target_value)\n",
    "            \n",
    "            # Write in logs\n",
    "            log_file = os.path.join(output_dir, f\"{dataset_name}_depthseek_log.txt\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"Max observed y: {y_max:.2f}\\n\")\n",
    "                f.write(f\"Depthseek Model params: a={a_est_2:.3f}, m={m_est_2:.2f}\\n\")\n",
    "                f.write(f\"Predicted value at {target_value}M: {y_pred_2:.2f}\\n\")\n",
    "                f.write(f\"Deviation: {deviation_2:.2%}\\n\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"LW Model prediction: At the sequencing depth of {target_value}M reads, the estimated library complexity is {y_pred_1:.2f}\")\n",
    "            print(f\"Depthseek Model prediction: At the sequencing depth of {target_value}M reads, the estimated library complexity is {y_pred_2:.2f}\")\n",
    "            print(f\"LW Model deviation: {deviation_1: 0.2%}\")\n",
    "            print(f\"Depthseek Model deviation: {deviation_2:.2%}\")\n",
    "\n",
    "            # Draw the fitting curves of the two models\n",
    "            cm = 1/2.54\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12*cm, 6*cm))\n",
    "            plot_fitted_curve(X_train, y_train, m_est_1, a_est_2, m_est_2, \n",
    "                             scaler_X, scaler_y, label=dataset_name, color='#EB5B25',\n",
    "                             ax1=ax1, ax2=ax2)\n",
    "            \n",
    "            ax1.scatter(X_train_inv, y_train_inv, s=5, color='gray', alpha=0.3, label='Training Data')\n",
    "            ax2.scatter(X_train_inv, y_train_inv, s=5, color='gray', alpha=0.3, label='Training Data')\n",
    "            \n",
    "            for ax in [ax1, ax2]:\n",
    "                ax.set_xlim(0, 800)\n",
    "                ax.tick_params(axis='both', labelsize=6)\n",
    "                ax.grid(False)\n",
    "                ax.yaxis.get_offset_text().set_fontsize(6)\n",
    "                ax.legend(loc='lower right', fontsize=4,edgecolor='white')\n",
    "                ax.spines['top'].set_visible(False)\n",
    "                ax.spines['right'].set_visible(False)\n",
    "            \n",
    "            ax1.set_xlabel('Sequencing Depth (Million Reads)', fontsize=6)\n",
    "            ax1.set_ylabel('Library Complexity (Read Counts)', fontsize=6)\n",
    "            ax2.set_xlabel('')\n",
    "            ax2.set_ylabel('')\n",
    "\n",
    "            ax1.text(0.5, 1.075, f'LW Model (Deviation: {deviation_1:.2%})', \n",
    "                 fontsize=6, ha='center', va='bottom', transform=ax1.transAxes)\n",
    "            ax2.text(0.5, 1.075, f'Depthseek Model (Deviation: {deviation_2:.2%})', \n",
    "                 fontsize=6, ha='center', va='bottom', transform=ax2.transAxes)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            output_svg = os.path.join(output_dir, f\"{dataset_name}_both_fit.svg\")\n",
    "            plt.savefig(output_svg, format='svg', bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Write in logs\n",
    "            log_file = os.path.join(output_dir, f\"{dataset_name}_both_log.txt\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"Max observed y: {y_max:.2f}\\n\")\n",
    "                f.write(f\"LW Model params: m={m_est_1:.2f}\\n\")\n",
    "                f.write(f\"Depthseek Model params: a={a_est_2:.3f}, m={m_est_2:.2f}\\n\")\n",
    "                f.write(f\"Predicted value at {target_value}M (LW): {y_pred_1:.2f}\\n\")\n",
    "                f.write(f\"Predicted value at {target_value}M (Depthseek): {y_pred_2:.2f}\\n\")\n",
    "                f.write(f\"Deviation (LW): {deviation_1:.2%}\\n\")\n",
    "                f.write(f\"Deviation (Depthseek): {deviation_2:.2%}\\n\")\n",
    "    \n",
    "    # Inverse mode (y -> x)\n",
    "    else:\n",
    "        target_y = target_value\n",
    "        \n",
    "        # Predict the value of x using the inverse function\n",
    "        predicted_x1 = inverse_function_1(target_y, m_est_1)\n",
    "        predicted_x2 = inverse_function_2(target_y, a_est_2, m_est_2)\n",
    "        \n",
    "        # Evaluate the performance of inverse functions on the training set\n",
    "        train_x_pred_1 = inverse_function_1(y_train_inv, m_est_1)\n",
    "        train_x_pred_2 = inverse_function_2(y_train_inv, a_est_2, m_est_2)\n",
    "        deviation_1 = calculate_inverse_deviation(X_train_inv, train_x_pred_1)\n",
    "        deviation_2 = calculate_inverse_deviation(X_train_inv, train_x_pred_2)\n",
    "        \n",
    "        # Evaluate the performance of inverse functions on the test set\n",
    "        test_x_pred_1 = inverse_function_1(y_test_inv, m_est_1)\n",
    "        test_x_pred_2 = inverse_function_2(y_test_inv, a_est_2, m_est_2)\n",
    "        test_deviation_1 = calculate_inverse_deviation(X_test_inv, test_x_pred_1)\n",
    "        test_deviation_2 = calculate_inverse_deviation(X_test_inv, test_x_pred_2)\n",
    "        \n",
    "        # Select the output based on the model type\n",
    "        if model_type == \"lw\":\n",
    "            print(f\"LW Model inverse inference: To achieve the library complexity of {target_y}, \\\n",
    "                  it is expected to require the sequencing depth of {predicted_x1:.2f}M reads \")\n",
    "            print(f\"LW Model training set deviation: {deviation_1: 0.2%}\")\n",
    "            print(f\"LW Model test set bias: {test_deviation_1: 1.2%}\")\n",
    "\n",
    "            # Write in logs\n",
    "            log_file = os.path.join(output_dir, f\"{dataset_name}_lw_inverse_log.txt\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"Max observed y: {y_max:.2f}\\n\")\n",
    "                f.write(f\"LW Model params: m={m_est_1:.2f}\\n\")\n",
    "                f.write(f\"Target Y value: {target_y:.2f}\\n\")\n",
    "                f.write(f\"Predicted X (LW Model): {predicted_x1:.2f}\\n\")\n",
    "                f.write(f\"Train Deviation (LW Model): {deviation_1:.2%}\\n\")\n",
    "                f.write(f\"Test Deviation (LW Model): {test_deviation_1:.2%}\\n\")\n",
    "                \n",
    "        elif model_type == \"depthseek\":\n",
    "            print(f\"Depthseek Model inverse inference: To achieve the library complexity of {target_y}, \\\n",
    "                  it is expected to require the sequencing depth of {predicted_x2:.2f}M reads \")\n",
    "            print(f\"Depthseek Model training set deviation: {deviation_2:.2%}\")\n",
    "            print(f\"Depthseek Model test set bias: {test_deviation_2:.2%}\")\n",
    "\n",
    "            # Write in logs\n",
    "            log_file = os.path.join(output_dir, f\"{dataset_name}_depthseek_inverse_log.txt\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"Max observed y: {y_max:.2f}\\n\")\n",
    "                f.write(f\"Depthseek Model params: a={a_est_2:.3f}, m={m_est_2:.2f}\\n\")\n",
    "                f.write(f\"Target Y value: {target_y:.2f}\\n\")\n",
    "                f.write(f\"Predicted X (Depthseek Model): {predicted_x2:.2f}\\n\")\n",
    "                f.write(f\"Train Deviation (Depthseek Model): {deviation_2:.2%}\\n\")\n",
    "                f.write(f\"Test Deviation (Depthseek Model): {test_deviation_2:.2%}\\n\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"LW Model reverse inference: To achieve the library complexity of {target_y}, \\\n",
    "                  it is expected to require the sequencing depth of {predicted_x1:.2f}M reads \")\n",
    "            print(f\"Depthseek Model reverse inference: To achieve the library complexity of {target_y}, \\\n",
    "                  it is expected to require the sequencing depth of {predicted_x2:.2f}M reads \")\n",
    "            print(f\"LW Model training set deviation: {deviation_1: 0.2%}\")\n",
    "            print(f\"Depthseek Model training set deviation: {deviation_2:.2%}\")\n",
    "            print(f\"LW Model test set bias: {test_deviation_1: 1.2%}\")\n",
    "            print(f\"Depthseek Model test set bias: {test_deviation_2:.2%}\")\n",
    "\n",
    "            # Write in logs\n",
    "            log_file = os.path.join(output_dir, f\"{dataset_name}_both_inverse_log.txt\")\n",
    "            with open(log_file, 'w') as f:\n",
    "                f.write(f\"Dataset: {dataset_name}\\n\")\n",
    "                f.write(f\"Max observed y: {y_max:.2f}\\n\")\n",
    "                f.write(f\"LW Model params: m={m_est_1:.2f}\\n\")\n",
    "                f.write(f\"Depthseek Model params: a={a_est_2:.3f}, m={m_est_2:.2f}\\n\")\n",
    "                f.write(f\"Target Y value: {target_y:.2f}\\n\")\n",
    "                f.write(f\"Predicted X (LW Model): {predicted_x1:.2f}\\n\")\n",
    "                f.write(f\"Predicted X (Depthseek Model): {predicted_x2:.2f}\\n\")\n",
    "                f.write(f\"Train Deviation (LW Model): {deviation_1:.2%}\\n\")\n",
    "                f.write(f\"Train Deviation (Depthseek Model): {deviation_2:.2%}\\n\")\n",
    "                f.write(f\"Test Deviation (LW Model): {test_deviation_1:.2%}\\n\")\n",
    "                f.write(f\"Test Deviation (Depthseek Model): {test_deviation_2:.2%}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Data Analysis and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start predictive analysis...\")\n",
    "run_prediction_analysis(final_output_file, output_dir, prediction_mode, target_value, model_type)\n",
    "print(\"Predictive analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result presentation(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = os.listdir(output_dir)\n",
    "print(\" Output file list :\")\n",
    "for file in output_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# If an HTML report is generated, you could display it\n",
    "html_report = os.path.join(output_dir, f\"{os.path.basename(final_output_file).replace('.txt', '')}_report.html\")\n",
    "if os.path.exists(html_report):\n",
    "    display(HTML(f'<a href=\"{html_report}\" target=\"_blank\">View HTML report</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit at full depth (0 to 400) and plot the repetition rate growth curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and save the results from the maximum sequencing depth of the current input +1 to 400 million\n",
    "def predict_to_400M(input_file, output_dir):\n",
    "    \"\"\"Predict and save the results from the maximum sequencing depth of the current input +1 to 400 million\"\"\"\n",
    "    # Load data\n",
    "    data = np.loadtxt(input_file)\n",
    "    X_original = data[:, 0]\n",
    "    y_original = data[:, 1]\n",
    "    \n",
    "    # Get the current maximum depth\n",
    "    max_depth = int(np.max(X_original))\n",
    "    y_max = np.max(y_original)\n",
    "    \n",
    "    # Estimate the parameters using the Depthseek model\n",
    "    _, a_est, m_est = estimate_parameters(X_original, y_original, y_max)\n",
    "    \n",
    "    # Predict the values from max_depth+1 to 400\n",
    "    predict_depths = np.arange(max_depth + 1, 401)\n",
    "    predicted_values = custom_function_2(predict_depths, a_est, m_est)\n",
    "    \n",
    "    # Merge the original data and the predicted data\n",
    "    combined_depths = np.concatenate([X_original, predict_depths])\n",
    "    combined_values = np.concatenate([y_original, predicted_values])\n",
    "    \n",
    "    # Save the result of the current input +1 to 400 million\n",
    "    dataset_name = os.path.basename(input_file).replace('.txt', '')\n",
    "    output_file = os.path.join(output_dir, f\"{dataset_name}_prediction_to_400M.txt\")\n",
    "    np.savetxt(output_file, np.column_stack((combined_depths, combined_values)), \n",
    "               fmt=['%.1f', '%.1f'], delimiter='\\t')\n",
    "    \n",
    "    print(f\"The prediction is completed and the result is saved to: {output_file}\")\n",
    "    print(f\"The number of input data points: {len(X_original)}\")\n",
    "    print(f\"The number of predicted data points: {len(predict_depths)}\")\n",
    "    print(f\"Total number of data points: {len(combined_depths)}\")\n",
    "\n",
    "# Run the function predicted to 400M\n",
    "print(\"Start predicting to a depth of 400 meters...\")\n",
    "predict_to_400M(final_output_file, output_dir)\n",
    "print(\"Prediction to a depth of 400 meters is completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the curve with the output file above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = os.path.basename(input_file).replace('.txt', '')\n",
    "fit_dup_Curve_input = os.path.join(output_dir, f\"{dataset_name}_prediction_to_400M.txt\")\n",
    "output_file = os.path.join(output_dir, f\"{dataset_name}_Dup_Rate_plot.svg\")\n",
    "\n",
    "rcParams['font.family'] = 'Arial'\n",
    "rcParams['font.size'] = 8\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def calculate_duplicate_rate(row):\n",
    "    return (row[0] * 1e6 - row[1]) / (row[0] * 1e6)\n",
    "\n",
    "def plot_single_sample(file_path, output_path):\n",
    "    fig, ax = plt.subplots(figsize=(6/2.54, 6/2.54))\n",
    "    \n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['x', 'y'])\n",
    "    df['duplicate_rate'] = df.apply(calculate_duplicate_rate, axis=1)\n",
    "    \n",
    "    ax.plot(df['x'], df['duplicate_rate'], color='#2ca02c', linewidth=1.5)\n",
    "    \n",
    "    ax.set_xticks([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])\n",
    "    ax.set_yticks([0.0, 0.05, 0.1, 0.15])\n",
    "    ax.tick_params(axis='both', length=2, labelsize=5)\n",
    "    \n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_ylim(0.05, 0.15)\n",
    "    \n",
    "    ax.set_xlabel('Sequencing Depth (Million Reads)', fontsize=8)\n",
    "    ax.set_ylabel('Duplication Rate', fontsize=8)\n",
    "    \n",
    "    ax.set_title(f'The Duplication Rate Growth Curve\\nSample \"{dataset_name}\"', fontsize=8)\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    \n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.savefig(output_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_single_sample(fit_dup_Curve_input, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Depthseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
